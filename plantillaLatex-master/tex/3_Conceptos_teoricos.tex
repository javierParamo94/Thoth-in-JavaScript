\capitulo{3}{Conceptos teóricos}

En mi experiencia como profesor de programación, diseño 3D y robótica para alumnos de primaria, lo primero que les enseñábamos era a definir la programación como el lenguaje de comunicación entre nosotros, los humanos, y los ordenadores o los robots. Todo esto pertenece a los conceptos más puramente teóricos sobre la computación y que es, en sí, la base de la informática. 

Pues bien, a <<grosso modo>> el concepto es el mismo ahora. Como cada uno, hombre y máquina, <<habla>> un idioma diferente se debe establecer un lenguaje que sea la vía de comunicación entre ambos. 

Para poder entender todo lo que se va a tratar a continuación, debo explicar primero el concepto de \textit{autómata} o \textit{máquina abstracta}. Un autómata es un modelo matemático o un dispositivo teórico que recibe una cadena de símbolos como entrada y que al procesarla, genera un cambio de estado produciendo una salida determinada. Esta salida pude reconocer palabras y determinar si la entrada pertenece a un determinado lenguaje o no. En el símil anterior digamos que es el corrector, que cuando escribe el humano algo determina si esta bien escrito o no porque no lo va a poder entender el ordenador. 

Entonces ya sabemos que para que haya comunicación entre un ordenador o robot y un humano, tiene que haber un lenguaje y un autómata. Pero también algo más: una gramática.

\section{¿Qué es una gramática y para qué sirve?}

Una gramática formal es un mecanismo para la generación de cadenas de caracteres que son admitidas por un determinado lenguaje formal, y que utiliza un conjunto de reglas de formación. Por lo tanto, podemos entenderlo dentro del concepto de las ciencias de la computación y la lógica matemática. Las cadenas de caracteres resultantes son a su vez <<bien formadas>> cuando pertenecen al lenguaje formal con el que se trabaja \cite{aho1986compilers}.

¿Y porque es tan importante? Siguiendo con el ejemplo del principio, la gramática es la que va a determinar si lo que se introduce en el autómata es correcto o no. Un conjunto de reglas que nos indicará el por qué al juntar una serie de caracteres de una forma se van a poder entender.

Por otro lado, la denominación de la gramática formal desde un punto de vista más formal, de denomina como una cuádrupla compuesta por:

\begin{itemize}
	\item Un alfabeto de \textbf{símbolos terminales} o tókenes denominado con la letra griega $\Sigma$.
	\item $\mathcal{N}$ que es un alfabeto formado por \textbf{símbolos no terminales}.
	\item Un alfabeto de \textbf{producciones} denominado $\mathcal{P}$.
	\item Y por último un símbolo llamado \textbf{axioma} o símbolo inicial el cual $\mathcal{S} \in \mathcal{N}$.
\end{itemize}

El alfabeto total que compone la gramática esta formado, según lo anterior, por $\Sigma\cup\mathcal{N}$, es decir, por el conjunto de los símbolos terminales y no terminales.

Una \textbf{producción} tiene esta estructura y esta compuesto por un par ordenado de cadenas. \[x \rightarrow y\] A la parte izquierda se la denomina antecedente y la derecha es el consecuente. A las producciones también se las denomina reglas de derivación.

Pongamos un ejemplo de una gramática simple y veamos de que esta formada. Se suele utilizar el sistema de notación  \[x \rightarrow y\] \[z \rightarrow w\] para indicar una o varias producciones, en vez de \[(x, y) \in \mathcal{P} \] \[(z, w) \in \mathcal{P} \] siendo $\mathcal{P}$ el conjunto de producciones.

Por otro lado si hay más de una producción que comience con el mismo elemento la notación sería de esta forma \[ x \rightarrow y | z | w\] en lugar de ser \[ x \rightarrow y\] \[x \rightarrow z\] \[x \rightarrow w\].

Se denomina \textbf{derivación directa} cuando sea $x \rightarrow y$ una producción y $v, w \in \Sigma^{*}$ decimos que $w$ deriva directamente de $v$ ( $v \Rightarrow w$) si $\exists z, u \in \Sigma^{*}$ tales que $v=zxu, w=zyu$ y $x\rightarrow y$.

La \textbf{derivación}: sea $\Sigma$ un alfabeto, un conjunto de producciones de ese alfabeto denominada $\mathcal{P}$ y $v, w \in \Sigma^{*}$ decimos que $w$ deriva de $v$, si existen una cadena de derivación de longitud $n$ así $u_{0}, u_{1}, ..., u_{n} \in \Sigma^{*}$ tales que:
 \[ v = u_{0} \Rightarrow u_{1}\] \[ u_{1} \Rightarrow u_{2}\] \[...\] \[ u_{n-1} \Rightarrow u_{n} = w\]

Se habla de lenguaje generado por una gramática $G$ escrito así $L(G)$ y se define como el conjunto de todas las sentencias de la gramática $G$.  \[L(G) = x\in \Sigma^{*}:\mathcal{S} \Rightarrow^{+}x\]
De esta manera se dice que dos gramáticas son equivalentes $G_{1} \equiv G_{2}$ si generan el mismo lenguaje $L(G_{1})=L(G_{2})$.

Por último una gramática es recursiva en un símbolo no terminal $U$ cuando existe una forma sentencial de $U$ que contiene a $U$. \[U \Rightarrow^{+}xUy \textup{ donde } x,y\in (\Sigma \cup \mathcal{N})^{*} \]
Así pues la gramática será recursiva cuando lo sea para algún no terminal, si $x = \varepsilon$ la gramática es recursiva por la izquierda, y si $y = \varepsilon$ se dice que es recursiva por la derecha.

\section{Tipos de gramáticas }

Hay varios tipos de gramáticas según sus características.
\begin{itemize}
\item \textbf{Gramáticas de Chomsky}: las producciones tienen la forma \[u \rightarrow v \textup{ donde } u=xAy \in (\Sigma \cup \mathcal{N})^{+} \wedge A \in \mathcal{N} \wedge x, y, v \in (\Sigma \cup \mathcal{N})^{*}\]

Es posible demostrar que los lenguajes generados por una gramática de Chomsky de un grupo más restringido llamadas gramáticas con estructura de frase, es decir que ambas tienen la misma capacidad generativa. Las gramáticas de frase tienen la siguiente forma de producción:

\[xAy \rightarrow xvy \textup{ donde } x,y,v \in (\Sigma \cup \mathcal{N})^{*} \wedge A \in \mathcal{N}\]
\item \textbf{Gramaticas dependientes o sensibles al contexto}: Cuentas con las reglas de producción con forma  \[xAy \rightarrow xvy \textup{ donde } x, y \in (\Sigma \cup \mathcal{N})^{*} \wedge A \in \mathcal{N} \wedge v \in (\Sigma \cup \mathcal{N})^{+}\]
Este tipo de gramáticas el significado de $A$ depende del contexto o de la posición en la frase. El contexto sería entonces $x$ e $y$. La mayor parte de los lenguajes de ordenador pertenecen a este tipo. Además longitud de la parte derecha de las producciones es siempre mayor o igual que la de la parte izquierda.
\item \textbf{Gramáticas independientes del contexto}: las producciones de las gramáticas de este tipo son más restrictivas, de la forma: 
\[A \rightarrow v \textup{ donde } A \in \mathcal{N} \wedge v \in (\Sigma \cup \mathcal{N})^{*}\]
Como su propio nombre indica, el significado de $A$ es independiente de la posición en la que se encuentra. Una característica importante es que las derivaciones obtenidas al utilizarse esta gramática se pueden representar utilizando árboles.

\item \textbf{Gramáticas regulares}: es el grupo más restringido. Tienen la forma: \[A \rightarrow aB \wedge A \rightarrow b \textup{ llamadas gramáticas regulares a derechas } \]
\[A \rightarrow Ba \wedge A \rightarrow b \textup{ llamadas gramáticas regulares a izquierdas } \]
\[ \textup{ donde }A, B \in \mathcal{N} \wedge a,b \in \Sigma  \]
Ambas son equivalentes. Existe tambien una generalización de este tipo de gramáticas denominadas lineales con reglas de la forma: \[A \rightarrow wB \wedge A \rightarrow v \textup{ lineales a derechas } \]
\[A \rightarrow Bw \wedge A \rightarrow v \textup{ lineales a izquierdas } \]
\[ \textup{ donde }A, B \in \mathcal{N} \wedge w,v \in \Sigma^{*}  \]
que son totalmente equivalentes a las regulares normales, pero en muchos casos su notación es más adecuada.
\end{itemize}


\subsection{Subsecciones}

Además de secciones tenemos subsecciones.

\subsubsection{Subsubsecciones}

Y subsecciones. 


\section{Referencias}

Las referencias se incluyen en el texto usando cite \cite{wiki:latex}. Para citar webs, artículos o libros \cite{koza92}.


